from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer
import pandas as pd
from datasets import Dataset
import re
from sentence_transformers.losses import TripletLoss
from sentence_transformers.training_args import SentenceTransformerTrainingArguments, BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator
import tensorflow as tf
import torch
from sklearn.metrics.pairwise import cosine_similarity


model = SentenceTransformer('all-mpnet-base-v2')


embedding = model.encode(["Deep Learning", "Machine Learning"])
similarities = cosine_similarity([embedding[0]], [embedding[1]])[0]
print(similarities)


df_original = pd.read_csv("/kaggle/input/rjdb-csv-format/dev.csv")
df_preprocessed = pd.read_csv("/kaggle/input/rjdb-csv-format/dev.csv")
df_preprocessed_test = pd.read_csv("/kaggle/input/rjdb-csv-format/test.csv")
df_original = df_original.iloc[:, 0:3]
df_preprocessed = df_preprocessed.iloc[:, 0:3]
df_preprocessed_test = df_preprocessed_test.iloc[:, 0:3]


print(df_preprocessed.iloc[0,0])


df_preprocessed.head()


def clean_text(text):
    text = re.sub(r'#*', '', text)
    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


df_preprocessed["Job-Description"] = df_preprocessed["Job-Description"].apply(clean_text)
df_preprocessed["Resume-matched"] = df_preprocessed["Resume-matched"].apply(clean_text)
df_preprocessed["Resume-unmatched"] = df_preprocessed["Resume-unmatched"].apply(clean_text)

df_preprocessed_test["Job-Description"] = df_preprocessed_test["Job-Description"].apply(clean_text)
df_preprocessed_test["Resume-matched"] = df_preprocessed_test["Resume-matched"].apply(clean_text)
df_preprocessed_test["Resume-unmatched"] = df_preprocessed_test["Resume-unmatched"].apply(clean_text)


df_preprocessed.head()


df_original.iloc[200,0]


df_preprocessed.iloc[200,0]


anchors = [i.iloc[0] for n,i in df_preprocessed.iterrows()]
positives = [i.iloc[1] for n,i in df_preprocessed.iterrows()]
negatives = [i.iloc[2] for n,i in df_preprocessed.iterrows()]

anchors_test = [i.iloc[0] for n,i in df_preprocessed_test.iterrows()]
positives_test = [i.iloc[1] for n,i in df_preprocessed_test.iterrows()]
negatives_test = [i.iloc[2] for n,i in df_preprocessed_test.iterrows()]


train_dataset = Dataset.from_dict({
    "anchor": anchors,
    "positive": positives,
    "negative": negatives
})

dataset_test = Dataset.from_dict({
    "anchor": anchors_test,
    "positive": positives_test,
    "negative": negatives_test
})

dataset_split = dataset_test.train_test_split(test_size=0.4, seed=42, shuffle = True)

eval_dataset = dataset_split['train']
test_dataset = dataset_split['test']


loss = TripletLoss(model)


args = SentenceTransformerTrainingArguments(
    output_dir="/kaggle/working/",
    
    # Training
    num_train_epochs=1,  # Recommended: 2-3 epochs for 2.5k examples
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_ratio=0.1,
    
    # Sampling strategy for better negatives
    batch_sampler=BatchSamplers.NO_DUPLICATES,
    # Evaluation / Saving
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,

    # Logging
    logging_steps=10,  # More frequent logs can help for short training
    report_to=[],
    run_name="mpnet-base-triplet-finetune"
)


triplet_evaluator = TripletEvaluator(
    anchors=eval_dataset["anchor"],
    positives=eval_dataset["positive"],
    negatives=eval_dataset["negative"],
    name="Evaluation mpnet-v2",
)
triplet_evaluator(model)


test_evaluator = TripletEvaluator(
    anchors=test_dataset["anchor"],
    positives=test_dataset["positive"],
    negatives=test_dataset["negative"],
    name="Evaluation mpnet-v2",
)
test_evaluator(model)


# Access the underlying transformer model
transformer = model._first_module().auto_model

# Freeze all layers
for param in transformer.parameters():
    param.requires_grad = False

# Unfreeze last N transformer layers (e.g., last 2)
unfreeze_layers = 1
for i in range(1, unfreeze_layers + 1):
    for param in transformer.encoder.layer[-i].parameters():
        param.requires_grad = True



# Access the base transformer model
transformer = model._first_module().auto_model

# Check number of encoder layers
print(f"\nTransformer has {len(transformer.encoder.layer)} encoder layers.")

# Print which layers are frozen or not
for i, layer in enumerate(transformer.encoder.layer):
    status = all(not p.requires_grad for p in layer.parameters())
    print(f"Layer {i}: {'‚ùÑÔ∏è Frozen' if status else 'üî• Trainable'}")



def count_parameters(model):
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total:,}")
    print(f"Trainable parameters: {trainable:,}")
    print(f"Frozen parameters: {total - trainable:,}")



count_parameters(model)


trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    loss=loss,
    evaluator=triplet_evaluator,
)
trainer.train()


test_evaluator = TripletEvaluator(
    anchors=test_dataset["anchor"],
    positives=test_dataset["positive"],
    negatives=test_dataset["negative"],
    name="Evaluation mpnet-v2",
)
test_evaluator(model)


trainer.save_model("/kaggle/working/fine_tuned_mpnet")


import shutil
shutil.make_archive("/kaggle/working/fine_tuned_mpnet 4", 'zip', "/kaggle/working/fine_tuned_mpnet")
